# Equalized learning rate

The author initilize the weight by normal distribution $N\thicksim(0,1)$ for all layers. After each time those layers parameters are called, they scale it with the He's initializer constant. Still not understand and will comeback later.

https://towardsdatascience.com/progan-how-nvidia-generated-images-of-unprecedented-quality-51c98ec2cbd2#:~:text=Pixel%20Normalization&text=This%20%E2%80%9Cpixelnorm%E2%80%9D%20layer%20has%20no,out%20of%20control%20during%20training.


https://medium.com/@EricKuy/progressive-gan-dd7cadb17dc4

https://theailearner.com/tag/mini-batch-standard-deviation/#:~:text=Minibatch%20Standard%20Deviation&text=To%20add%20a%20little%20variation,new%20activation%2C%20maps%20are%20created.

https://medium.com/@animeshsk3/the-unprecedented-effectiveness-of-progressive-growing-of-gans-37475c88afa3